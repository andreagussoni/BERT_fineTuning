{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gusso\\anaconda3\\envs\\hugging_torch_2\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "import pandas as pd\n",
    "from sklearn.utils import resample\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from optimum.bettertransformer import BetterTransformer\n",
    "import torch\n",
    "from pynvml import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.1.1\n",
      "Is CUDA enabled? True\n"
     ]
    }
   ],
   "source": [
    "print(\"Torch version:\",torch.__version__)\n",
    "\n",
    "print(\"Is CUDA enabled?\",torch.cuda.is_available())\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n",
    "\n",
    "def print_summary(result):\n",
    "    print(f\"Time: {result.metrics['train_runtime']:.2f}\")\n",
    "    print(f\"Samples/second: {result.metrics['train_samples_per_second']:.2f}\")\n",
    "    print_gpu_utilization()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset financial_phrasebank (C:/Users/Gusso/.cache/huggingface/datasets/financial_phrasebank/sentences_50agree/1.0.0/550bde12e6c30e2674da973a55f57edde5181d53f5a5a34c1531c53f93b7e141)\n",
      "100%|██████████| 1/1 [00:00<00:00, 500.04it/s]\n",
      "Loading cached processed dataset at C:\\Users\\Gusso\\.cache\\huggingface\\datasets\\financial_phrasebank\\sentences_50agree\\1.0.0\\550bde12e6c30e2674da973a55f57edde5181d53f5a5a34c1531c53f93b7e141\\cache-4069f8bdc2072a81.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Gusso\\.cache\\huggingface\\datasets\\financial_phrasebank\\sentences_50agree\\1.0.0\\550bde12e6c30e2674da973a55f57edde5181d53f5a5a34c1531c53f93b7e141\\cache-255302c37e6b94c9.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Gusso\\.cache\\huggingface\\datasets\\financial_phrasebank\\sentences_50agree\\1.0.0\\550bde12e6c30e2674da973a55f57edde5181d53f5a5a34c1531c53f93b7e141\\cache-913aca77f5bc0114.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['sentence', 'label'],\n",
      "    num_rows: 604\n",
      "})\n",
      "Dataset({\n",
      "    features: ['sentence', 'label'],\n",
      "    num_rows: 1208\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"financial_phrasebank\", 'sentences_50agree')\n",
    "\n",
    "neutrals = dataset.filter(lambda example: example[\"label\"] == 1)\n",
    "positives = dataset.filter(lambda example: example[\"label\"] == 2)\n",
    "negatives = dataset.filter(lambda example: example[\"label\"] == 0)\n",
    "\n",
    "short_positives = positives['train'].select(range(len(negatives['train'])))\n",
    "\n",
    "print(short_positives)\n",
    "\n",
    "downsized_dataset = concatenate_datasets([short_positives, negatives['train']])\n",
    "\n",
    "print(downsized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\Gusso\\.cache\\huggingface\\datasets\\financial_phrasebank\\sentences_50agree\\1.0.0\\550bde12e6c30e2674da973a55f57edde5181d53f5a5a34c1531c53f93b7e141\\cache-9ab259d81c7af6d1.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sentence', 'label'],\n",
      "        num_rows: 1087\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['sentence', 'label'],\n",
      "        num_rows: 121\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "def replace_values(example):\n",
    "    # Replace values in the label column\n",
    "    if \"label\" in example:\n",
    "        example[\"label\"] = 1 if example[\"label\"] == 2 else example[\"label\"]\n",
    "    return example\n",
    "\n",
    "# Use the map function to apply the custom mapping\n",
    "downsized_dataset = downsized_dataset.map(replace_values)\n",
    "downsized_dataset = downsized_dataset.train_test_split(test_size=0.1, shuffle=True)\n",
    "print(downsized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"sentence\"], padding='max_length', truncation=True)\n",
    "\n",
    "tokenized_datasets = downsized_dataset['train'].map(tokenize_function, batched=True)\n",
    "tokenized_datasets_eval = downsized_dataset['test'].map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1340583944\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-large-uncased\", load_in_8bit=True)\n",
    "\n",
    "# model = model.to(0)\n",
    "\n",
    "print(model.get_memory_footprint())\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets,\n",
    "    eval_dataset=tokenized_datasets_eval,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/204 [00:00<?, ?it/s]c:\\Users\\Gusso\\anaconda3\\envs\\hugging_torch_2\\lib\\site-packages\\torch\\utils\\checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "                                                \n",
      " 33%|███▎      | 68/204 [02:55<05:33,  2.45s/it]c:\\Users\\Gusso\\anaconda3\\envs\\hugging_torch_2\\lib\\site-packages\\torch\\utils\\checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': nan, 'eval_accuracy': 0.47107438016528924, 'eval_runtime': 7.9965, 'eval_samples_per_second': 15.132, 'eval_steps_per_second': 7.628, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 67%|██████▋   | 136/204 [05:55<02:48,  2.48s/it]c:\\Users\\Gusso\\anaconda3\\envs\\hugging_torch_2\\lib\\site-packages\\torch\\utils\\checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': nan, 'eval_accuracy': 0.47107438016528924, 'eval_runtime': 7.975, 'eval_samples_per_second': 15.172, 'eval_steps_per_second': 7.649, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      "100%|██████████| 204/204 [08:58<00:00,  2.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': nan, 'eval_accuracy': 0.47107438016528924, 'eval_runtime': 7.708, 'eval_samples_per_second': 15.698, 'eval_steps_per_second': 7.914, 'epoch': 3.0}\n",
      "{'train_runtime': 538.0141, 'train_samples_per_second': 6.061, 'train_steps_per_second': 0.379, 'train_loss': 56087577897000.16, 'epoch': 3.0}\n",
      "Time: 538.01\n",
      "Samples/second: 6.06\n"
     ]
    },
    {
     "ename": "NVMLError_LibraryNotFound",
     "evalue": "NVML Shared Library Not Found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Gusso\\anaconda3\\envs\\hugging_torch_2\\lib\\site-packages\\pynvml.py:641\u001b[0m, in \u001b[0;36m_LoadNvmlLibrary\u001b[1;34m()\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[39mif\u001b[39;00m (sys\u001b[39m.\u001b[39mplatform[:\u001b[39m3\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mwin\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    639\u001b[0m     \u001b[39m# cdecl calling convention\u001b[39;00m\n\u001b[0;32m    640\u001b[0m     \u001b[39m# load nvml.dll from %ProgramFiles%/NVIDIA Corporation/NVSMI/nvml.dll\u001b[39;00m\n\u001b[1;32m--> 641\u001b[0m     nvmlLib \u001b[39m=\u001b[39m CDLL(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(os\u001b[39m.\u001b[39;49mgetenv(\u001b[39m\"\u001b[39;49m\u001b[39mProgramFiles\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mC:/Program Files\u001b[39;49m\u001b[39m\"\u001b[39;49m), \u001b[39m\"\u001b[39;49m\u001b[39mNVIDIA Corporation/NVSMI/nvml.dll\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[0;32m    642\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    643\u001b[0m     \u001b[39m# assume linux\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Gusso\\anaconda3\\envs\\hugging_torch_2\\lib\\ctypes\\__init__.py:374\u001b[0m, in \u001b[0;36mCDLL.__init__\u001b[1;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[39mif\u001b[39;00m handle \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 374\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle \u001b[39m=\u001b[39m _dlopen(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_name, mode)\n\u001b[0;32m    375\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: Could not find module 'C:\\Program Files\\NVIDIA Corporation\\NVSMI\\nvml.dll' (or one of its dependencies). Try using the full path with constructor syntax.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNVMLError_LibraryNotFound\u001b[0m                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\ai_project\\bert_large_cuda_optimum_quantized_8bit_adam.ipynb Cell 8\u001b[0m line \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/ai_project/bert_large_cuda_optimum_quantized_8bit_adam.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m result \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/ai_project/bert_large_cuda_optimum_quantized_8bit_adam.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m print_summary(result)\n",
      "\u001b[1;32mc:\\ai_project\\bert_large_cuda_optimum_quantized_8bit_adam.ipynb Cell 8\u001b[0m line \u001b[0;36mprint_summary\u001b[1;34m(result)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/ai_project/bert_large_cuda_optimum_quantized_8bit_adam.ipynb#X10sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTime: \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m.\u001b[39mmetrics[\u001b[39m'\u001b[39m\u001b[39mtrain_runtime\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/ai_project/bert_large_cuda_optimum_quantized_8bit_adam.ipynb#X10sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSamples/second: \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m.\u001b[39mmetrics[\u001b[39m'\u001b[39m\u001b[39mtrain_samples_per_second\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/ai_project/bert_large_cuda_optimum_quantized_8bit_adam.ipynb#X10sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m print_gpu_utilization()\n",
      "\u001b[1;32mc:\\ai_project\\bert_large_cuda_optimum_quantized_8bit_adam.ipynb Cell 8\u001b[0m line \u001b[0;36mprint_gpu_utilization\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/ai_project/bert_large_cuda_optimum_quantized_8bit_adam.ipynb#X10sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprint_gpu_utilization\u001b[39m():\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/ai_project/bert_large_cuda_optimum_quantized_8bit_adam.ipynb#X10sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     nvmlInit()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/ai_project/bert_large_cuda_optimum_quantized_8bit_adam.ipynb#X10sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     handle \u001b[39m=\u001b[39m nvmlDeviceGetHandleByIndex(\u001b[39m0\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/ai_project/bert_large_cuda_optimum_quantized_8bit_adam.ipynb#X10sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     info \u001b[39m=\u001b[39m nvmlDeviceGetMemoryInfo(handle)\n",
      "File \u001b[1;32mc:\\Users\\Gusso\\anaconda3\\envs\\hugging_torch_2\\lib\\site-packages\\pynvml.py:608\u001b[0m, in \u001b[0;36mnvmlInit\u001b[1;34m()\u001b[0m\n\u001b[0;32m    607\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnvmlInit\u001b[39m():\n\u001b[1;32m--> 608\u001b[0m     _LoadNvmlLibrary()\n\u001b[0;32m    610\u001b[0m     \u001b[39m#\u001b[39;00m\n\u001b[0;32m    611\u001b[0m     \u001b[39m# Initialize the library\u001b[39;00m\n\u001b[0;32m    612\u001b[0m     \u001b[39m#\u001b[39;00m\n\u001b[0;32m    613\u001b[0m     fn \u001b[39m=\u001b[39m _nvmlGetFunctionPointer(\u001b[39m\"\u001b[39m\u001b[39mnvmlInit_v2\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Gusso\\anaconda3\\envs\\hugging_torch_2\\lib\\site-packages\\pynvml.py:646\u001b[0m, in \u001b[0;36m_LoadNvmlLibrary\u001b[1;34m()\u001b[0m\n\u001b[0;32m    644\u001b[0m         nvmlLib \u001b[39m=\u001b[39m CDLL(\u001b[39m\"\u001b[39m\u001b[39mlibnvidia-ml.so.1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    645\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m ose:\n\u001b[1;32m--> 646\u001b[0m     _nvmlCheckReturn(NVML_ERROR_LIBRARY_NOT_FOUND)\n\u001b[0;32m    647\u001b[0m \u001b[39mif\u001b[39;00m (nvmlLib \u001b[39m==\u001b[39m \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    648\u001b[0m     _nvmlCheckReturn(NVML_ERROR_LIBRARY_NOT_FOUND)\n",
      "File \u001b[1;32mc:\\Users\\Gusso\\anaconda3\\envs\\hugging_torch_2\\lib\\site-packages\\pynvml.py:310\u001b[0m, in \u001b[0;36m_nvmlCheckReturn\u001b[1;34m(ret)\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_nvmlCheckReturn\u001b[39m(ret):\n\u001b[0;32m    309\u001b[0m     \u001b[39mif\u001b[39;00m (ret \u001b[39m!=\u001b[39m NVML_SUCCESS):\n\u001b[1;32m--> 310\u001b[0m         \u001b[39mraise\u001b[39;00m NVMLError(ret)\n\u001b[0;32m    311\u001b[0m     \u001b[39mreturn\u001b[39;00m ret\n",
      "\u001b[1;31mNVMLError_LibraryNotFound\u001b[0m: NVML Shared Library Not Found"
     ]
    }
   ],
   "source": [
    "result = trainer.train()\n",
    "print_summary(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "##model = BetterTransformer.transform(model)\n",
    "\n",
    "trainer2 = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets,\n",
    "    eval_dataset=tokenized_datasets_eval,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (C:/Users/Gusso/.cache/huggingface/datasets/zeroshot___csv/zeroshot--twitter-financial-news-sentiment-ccca0f3c622c5b67/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "100%|██████████| 2/2 [00:00<00:00, 84.82it/s]\n",
      "Loading cached processed dataset at C:\\Users\\Gusso\\.cache\\huggingface\\datasets\\zeroshot___csv\\zeroshot--twitter-financial-news-sentiment-ccca0f3c622c5b67\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-b2d98c45395bc49c.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Gusso\\.cache\\huggingface\\datasets\\zeroshot___csv\\zeroshot--twitter-financial-news-sentiment-ccca0f3c622c5b67\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-98a6e9e55fa382e5.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Gusso\\.cache\\huggingface\\datasets\\zeroshot___csv\\zeroshot--twitter-financial-news-sentiment-ccca0f3c622c5b67\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-fe9caa88d8fabe99.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Gusso\\.cache\\huggingface\\datasets\\zeroshot___csv\\zeroshot--twitter-financial-news-sentiment-ccca0f3c622c5b67\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-9fbf952eb2a40f9a.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sentence', 'label'],\n",
      "        num_rows: 4846\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "validation_dataset = load_dataset('zeroshot/twitter-financial-news-sentiment')\n",
    "\n",
    "def filter_function(example):\n",
    "    return example[\"label\"] != 2\n",
    "\n",
    "# Use the filter method to remove records\n",
    "filtered_dataset = validation_dataset.filter(filter_function)\n",
    "\n",
    "def tokenize_function2(examples):\n",
    "    return tokenizer(examples[\"text\"], padding='max_length', truncation=True)\n",
    "\n",
    "tokenized_datasets_validation = filtered_dataset.map(tokenize_function2, batched=True)\n",
    "\n",
    "# Print the modified dataset\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 411/411 [00:52<00:00,  7.83it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': nan,\n",
       " 'eval_accuracy': 0.4221411192214112,\n",
       " 'eval_runtime': 52.7567,\n",
       " 'eval_samples_per_second': 15.581,\n",
       " 'eval_steps_per_second': 7.79}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer2.evaluate(eval_dataset=tokenized_datasets_validation['validation'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hugging_torch_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
